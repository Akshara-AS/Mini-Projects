{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335608cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1ab9d",
   "metadata": {},
   "source": [
    "# A. Basic Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c5dc2",
   "metadata": {},
   "source": [
    "## 1. Lowering Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae1bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The steps should be explained ONE by one\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Lowered Sentence: the steps should be explained one by one\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The steps should be explained ONE by one\" \n",
    "sentence_lower=str(sentence).lower()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Lowered Sentence:\", sentence_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc310728",
   "metadata": {},
   "source": [
    "## 2. Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c71514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7605ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I got $ 100 today, this is my happiest day !\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without Punctuations: I got 100 today, this is my happiest day\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I got $ 100 today, this is my happiest day !\"\n",
    "without_punc=[word for word in sentence.split(\" \") if word not in list(punc)]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without Punctuations:\", \" \".join(without_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bbfc8",
   "metadata": {},
   "source": [
    "## 3. Removing Special Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265684bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Find the remainder when  math          math  is divided by       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence=\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\" \n",
    "sentence_clean=re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", sentence_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594b5e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Area of trapezoid is : A = ½ (a + b) h\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Area of trapezoid is   A      a   b  h\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence=\"Area of trapezoid is : A = ½ (a + b) h\" \n",
    "sentence_clean=re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168719ef",
   "metadata": {},
   "source": [
    "## 4. Removal of HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293c134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: <h3 style=\"color:red; font-family:Arial Black\">This is NLP</h3>\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: This is NLP\n"
     ]
    }
   ],
   "source": [
    "sentence='''<h3 style=\"color:red; font-family:Arial Black\">This is NLP</h3>''' \n",
    "clean_sentence=re.sub(\"<.*?>\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87e1fb",
   "metadata": {},
   "source": [
    "## 5. Removing URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc45d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I visited https://github.com/Suthir24 \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: I visited  \n"
     ]
    }
   ],
   "source": [
    "sentence=\"I visited https://github.com/Suthir24 \"\n",
    "clean_sentence=re.sub(\"(http|https|www)\\S+\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204c66c",
   "metadata": {},
   "source": [
    "## 6. Removing Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85120419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I'm     Learning     NLP\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: I'm Learning NLP\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I'm     Learning     NLP\" \n",
    "clean_sentence=re.sub(\" +\",\" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22374fb",
   "metadata": {},
   "source": [
    "## 7. Expanding Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f254f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bdf188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I can't wait to see you\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clear Sentence: I cannot wait to see you\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I can't wait to see you\"\n",
    "clear_sentence=contractions.fix(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clear Sentence:\", clear_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87174b2f",
   "metadata": {},
   "source": [
    "## 8. Text Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b748c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Proteens are essential building muscles\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Correct Sentence: Proteins are essential building muscles\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence=\"Proteens are essential building muscles\"\n",
    "textblob=TextBlob(sentence)\n",
    "correct_sentence=textblob.correct()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Correct Sentence:\", correct_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b4427",
   "metadata": {},
   "source": [
    "\n",
    "# B. Advanced Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fe6a9",
   "metadata": {},
   "source": [
    "# 1. Apply Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c53bfc",
   "metadata": {},
   "source": [
    "## a. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64667cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6d570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: My name is Suthir. I work for an MNC in ANtartica. It is very hot out there.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence Tokens: ['My name is Suthir.', 'I work for an MNC in ANtartica.', 'It is very hot out there.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence='''My name is Suthir. I work for an MNC in ANtartica. It is very hot out there.'''\n",
    "tokens=sent_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c2a26",
   "metadata": {},
   "source": [
    "## b. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad99139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Roger Federer is the greatest player of all time in Tennis\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Roger', 'Federer', 'is', 'the', 'greatest', 'player', 'of', 'all', 'time', 'in', 'Tennis']\n"
     ]
    }
   ],
   "source": [
    "sentence='''Roger Federer is the greatest player of all time in Tennis'''\n",
    "tokens=sentence.split(\" \")\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0998c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Is Roger Federer is the greatest player of all time in Tennis ?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Is', 'Roger', 'Federer', 'is', 'the', 'greatest', 'player', 'of', 'all', 'time', 'in', 'Tennis', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence='''Is Roger Federer is the greatest player of all time in Tennis ?'''\n",
    "tokens=word_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308dbe5",
   "metadata": {},
   "source": [
    "### We can easily see the difference, when we tokenize using string method, it will consider all the special characters & punctuation attached to a word as a part of that word, but when we tokenize using NLTK word_tokenizer it consider those special characters & punctuation as a seperate toke."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be58d58",
   "metadata": {},
   "source": [
    "## c. Sub-Word(n-gram character) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d03a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1451c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The Laver Cup is usually played between Europe vs The World \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "N-gram Tokens: [('The', 'Laver', 'Cup'), ('Laver', 'Cup', 'is'), ('Cup', 'is', 'usually'), ('is', 'usually', 'played'), ('usually', 'played', 'between'), ('played', 'between', 'Europe'), ('between', 'Europe', 'vs'), ('Europe', 'vs', 'The'), ('vs', 'The', 'World'), ('The', 'World', '')]\n"
     ]
    }
   ],
   "source": [
    "sentence='''The Laver Cup is usually played between Europe vs The World '''\n",
    "n_gram_tokens=list(ngrams((sentence.split(\" \")), n=3))\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"N-gram Tokens:\", n_gram_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79433c90",
   "metadata": {},
   "source": [
    "## 2. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea8b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/suka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d45f2af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stop Words in English= 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopwords_en=stopwords.words(\"english\")\n",
    "print(\"Total Stop Words in English=\", len(stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362e2e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with StopWOrds: Lion is considered as the king of the jungle\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without StopWOrds: Lion considered king jungle\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Lion is considered as the king of the jungle\"\n",
    "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_en] \n",
    "print(\"Sentence with StopWOrds:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without StopWOrds:\", \" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fc8b4",
   "metadata": {},
   "source": [
    "## 3. Apply Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b467e",
   "metadata": {},
   "source": [
    "### Types of Stemmer in NLP:    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9306f4d",
   "metadata": {},
   "source": [
    "###    a. Porter Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2a6d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: running jumps jumping chocolates better goodness\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: run jump jump chocol better good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "sentence= \"running jumps jumping chocolates better goodness\"\n",
    "porter_stem=[porter.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(porter_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5ee29",
   "metadata": {},
   "source": [
    "###    b. SnowBall Stemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e42a2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: running jumps jumping chocolates better goodness\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: run jump jump chocol better good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball=SnowballStemmer(language=\"english\")\n",
    "sentence=\"running jumps jumping chocolates better goodness\"\n",
    "snowball_stem=[snowball.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(snowball_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259fcc1c",
   "metadata": {},
   "source": [
    "###    c.Lancaster Stemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea398e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: running jumps jumping chocolates better goodness \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: run jump jump chocol bet good \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "sentence=\"running jumps jumping chocolates better goodness \"\n",
    "lancaster_stem=[lancaster.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe48d8d",
   "metadata": {},
   "source": [
    "###    d. Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6472d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: running jumps jumping chocolates better kindness\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: runn jump jump chocolate better kindnes\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex=RegexpStemmer(regexp=\"ing$|s$|e$\", min=0)\n",
    "sentence=\"running jumps jumping chocolates better kindness\"\n",
    "regex_stem=[regex.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(regex_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882d298",
   "metadata": {},
   "source": [
    "### We can see the Stemmer is not able to produce the correct outcomes thus we use Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7dcaa",
   "metadata": {},
   "source": [
    "## 4. Apply Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7132f9",
   "metadata": {},
   "source": [
    "## Types of Lemmatization in NLP:    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1139c0",
   "metadata": {},
   "source": [
    "### a. Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "994ebafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/suka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52976fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/suka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdfadb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The cats are playing with the mice and the birds.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: The cat be play with the mice and the birds.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "sentence=\"The cats are playing with the mice and the birds.\" \n",
    "sentence_lemma=[lemma.lemmatize(word, 'v') for word in sentence.split(\" \")] \n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07022c",
   "metadata": {},
   "source": [
    "### b. TextBlob Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "411023fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The bats are hanging on their feet in upright positions\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: The bat are hanging on their foot in upright position\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "sentence=\"The bats are hanging on their feet in upright positions\" \n",
    "sent=TextBlob(sentence)\n",
    "texblob_lemma=[w.lemmatize() for w in sent.words]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(texblob_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcaf13a",
   "metadata": {},
   "source": [
    "### c. More Advanced Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42995571",
   "metadata": {},
   "source": [
    "#### These Techniques are not used in all the tasks, these are problem specific. These techniques are mainly used in QA System(Question Answer), Word Sense Disambiguiation etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91dcfe",
   "metadata": {},
   "source": [
    "## 1. POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0b8e8",
   "metadata": {},
   "source": [
    "### Adding a Part of Speech tags to every word in the corpus is called POS tagging. If we want to perform POS tagging then no need to remove stopwords. This is one of the essential steps in the text analysis where we know the sentence structure and which word is connected to the other, which word is rooted from which, eventually, to figure out hidden connections between words which can later boost the performance of our Machine Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd8e15",
   "metadata": {},
   "source": [
    "##### The Pos taglist is given:\n",
    "* CC - Coordinating conjunction\n",
    "* CD - Cardinal number\n",
    "* DT - Determiner\n",
    "* EX - Existential there\n",
    "* FW - Foreign word\n",
    "* IN - Preposition or subordinating conjunction\n",
    "* JJ - Adjective\n",
    "* JJR -Adjective, comparative\n",
    "* JJS- Adjective, superlative\n",
    "* LS - List item marker\n",
    "* MD - Modal\n",
    "* NN - Noun, singular or mass\n",
    "* NNS- Noun, plural\n",
    "* NNP- Proper noun, singular\n",
    "* NNPS-    Proper noun, plural\n",
    "* PDT -Predeterminer\n",
    "* POS -Possessive ending\n",
    "* PRP -Personal pronoun\n",
    "* PRP -   Possessive pronoun\n",
    "* RB - Adverb\n",
    "* RBR -Adverb, comparative\n",
    "* RBS- Adverb, superlative\n",
    "* RP - Particle\n",
    "* SYM -Symbol\n",
    "* TO - to\n",
    "* UH - Interjection\n",
    "* VB - Verb, base form\n",
    "* VBD - Verb, past tense\n",
    "* VBG - Verb, gerund or present participle\n",
    "* VBN  - Verb, past participle\n",
    "* VBP- Verb, non-3rd person singular present\n",
    "* VBZ- Verb, 3rd person singular present\n",
    "* WDT- Wh-determiner\n",
    "* WP - Wh-pronoun\n",
    "* WP -Possessive wh-pronoun\n",
    "* WRB -Wh-adverb\n",
    "###### -- for WP and PRP if $ is used , it indicates possesive pronoun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2a0d",
   "metadata": {},
   "source": [
    "## POS Tagging can be performed using two Libraries\n",
    "###         a. POS Tagging using NLTK    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "833beaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9cf5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The || POS Tag: DT\n",
      "Word: quick || POS Tag: JJ\n",
      "Word: brown || POS Tag: NN\n",
      "Word: fox || POS Tag: NN\n",
      "Word: jumps || POS Tag: VBZ\n",
      "Word: over || POS Tag: IN\n",
      "Word: the || POS Tag: DT\n",
      "Word: lazy || POS Tag: JJ\n",
      "Word: dog || POS Tag: NN\n",
      "Word: . || POS Tag: .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "doc=word_tokenize(\"The quick brown fox jumps over the lazy dog.\") \n",
    "for i in range(len(doc)):\n",
    "    print(\"Word:\",pos_tag(doc)[i][0], \"||\", \"POS Tag:\", pos_tag(doc)[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1ce04",
   "metadata": {},
   "source": [
    "### b. POS Tagging using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ffbc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c66751f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS: PRON || POS Tag: WP || POS Tag Description: PRON\n",
      "Word: is || POS: AUX || POS Tag: VBZ || POS Tag Description: AUX\n",
      "Word: the || POS: DET || POS Tag: DT || POS Tag Description: DET\n",
      "Word: step || POS: NOUN || POS Tag: NN || POS Tag Description: NOUN\n",
      "Word: by || POS: ADP || POS Tag: IN || POS Tag Description: ADP\n",
      "Word: step || POS: NOUN || POS Tag: NN || POS Tag Description: NOUN\n",
      "Word: guide || POS: NOUN || POS Tag: NN || POS Tag Description: NOUN\n",
      "Word: to || POS: PART || POS Tag: TO || POS Tag Description: PART\n",
      "Word: invest || POS: VERB || POS Tag: VB || POS Tag Description: VERB\n",
      "Word: in || POS: ADP || POS Tag: IN || POS Tag Description: ADP\n",
      "Word: share || POS: NOUN || POS Tag: NN || POS Tag Description: NOUN\n",
      "Word: market || POS: NOUN || POS Tag: NN || POS Tag Description: NOUN\n",
      "Word: in || POS: ADP || POS Tag: IN || POS Tag Description: ADP\n",
      "Word: India || POS: PROPN || POS Tag: NNP || POS Tag Description: PROPN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"What is the step by step guide to invest in share market in India\")\n",
    "for word in doc:\n",
    "    print(\"Word:\", word.text, \"||\", \"POS:\", word.pos_, \"||\", \"POS Tag:\", word.tag_, \"||\", \"POS Tag Description:\", word.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f6b59",
   "metadata": {},
   "source": [
    "### Spacy is more powerful than NLTK. Spacy is faster and Grammatically accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e981df",
   "metadata": {},
   "source": [
    "## 2. NER Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b640a3",
   "metadata": {},
   "source": [
    "### Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as well as themes, topics, times, monetary values and percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27066eeb",
   "metadata": {},
   "source": [
    "## NER can be performed using two Libraries\n",
    "   ## a. NER using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15b99161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/suka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd322428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON John/NNP)\n",
      "(ORGANIZATION Doe/NNP)\n",
      "('CEO', 'NNP')\n",
      "('XYZ', 'NNP')\n",
      "('Corp', 'NNP')\n",
      "(GPE New/NNP York/NNP)\n"
     ]
    }
   ],
   "source": [
    "sentence=\"John Doe is the CEO of XYZ Corp in New York\"\n",
    "words=[word for word in sentence.split(\" \") if word not in stopwords_en] \n",
    "tagged_tokens=nltk.pos_tag(words)\n",
    "entities=nltk.ne_chunk(tagged_tokens)\n",
    "for entity in entities: print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaf759",
   "metadata": {},
   "source": [
    "## b. NER using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d27a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. ORG\n",
      "Steve Jobs PERSON\n",
      "Steve Wozniak PERSON\n",
      "April 1, 1976 DATE\n",
      "Cupertino GPE\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence=\"Apple Inc. was founded by Steve Jobs and Steve Wozniak on April 1, 1976, in Cupertino, California. It is a leading technology company.\" \n",
    "doc = nlp(sentence)\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67876a",
   "metadata": {},
   "source": [
    "### Spacy is a faster and more efficient library for NER. It provides a pre-trained NER model that is highly accurate than NLTK and can recognize a wide range of named entities. Additionally, SpaCy has more advanced features such as named entity linking and coreference resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8675109",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4add02a",
   "metadata": {},
   "source": [
    "# Text to Numerical Vector Conversion Techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37773e23",
   "metadata": {},
   "source": [
    "## Our Machine Learning and Deep Learning models take only numerical data as an input to train the model and do prediction, Hence it is necessary to perform conversion step to make texual data into equivalent numerical representation. There are many text to numerical vector conversion techniques, these techniques are,\n",
    "## 1. BOW(Bag Of Word): Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b505973",
   "metadata": {},
   "source": [
    "### It is a Collection of words represent a sentence with word count. Steps invloved in this process are Clean Text, Tookenize, Build Vocabulary and Generate Vecors. We can create vocabulory of size 1 to n using uni-ngram, bi- gram, n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73e9d11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (unique words):\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "BOW representation of the first document:\n",
      "[0 1 1 1 0 0 1 0 1]\n",
      "\n",
      "BOW representation of all documents:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus (collection of text documents)\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Step 1: Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit the vectorizer on the corpus and transform the data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# The output is a sparse matrix where each row represents a document,\n",
    "# and each column represents a unique word in the corpus.\n",
    "\n",
    "# Step 3: Print the feature names (unique words) found in the corpus\n",
    "print(\"Feature names (unique words):\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 4: Print the BOW representation of the first document\n",
    "print(\"\\nBOW representation of the first document:\")\n",
    "print(X.toarray()[0])\n",
    "\n",
    "# Step 5: Print the BOW representation of all the documents\n",
    "print(\"\\nBOW representation of all documents:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b708b",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "###   a. Simple Procedure and easy to implement.\n",
    "###   b. Easy to Understand\n",
    "## Disadvantages:\n",
    "###   a. Does not consider the symmentic meaning of the word.    b. Due to large vector size computational time is high.    c. Count Vectorizer Generates Spars matrix.\n",
    "###   d. Out of Vocabulary words are not captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c6533",
   "metadata": {},
   "source": [
    "## 2. TF-IDF(Term Frequence-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfdb28",
   "metadata": {},
   "source": [
    "## It is a Statistical method. It measures how important a term or word is within a document or setence relative to a collection of documents or Corpus. Words within a text document are transformed into importance numbers by a text vectorization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2658472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Step 1: Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the documents to obtain the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Print the vocabulary (unique words) and the TF-IDF matrix\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd53fa2",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "   ### a. Simple Procedure and easy to implement.\n",
    "   ### b. Easy to Understand\n",
    "   ### c. Here unlike BOW, weightage for those words is given high if that word occuring in that document but occuring less in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ecdd6",
   "metadata": {},
   "source": [
    "## Disadvantages:\n",
    "### a. It cannot assist in carrying semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861540ef",
   "metadata": {},
   "source": [
    "## 3. Word2Vec(Word to Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dd978",
   "metadata": {},
   "source": [
    "## It is a pre-trained word embedded model. Word2Vec creates vectors of the words that are distributed numerical representations of word features. These word features represents the context for the each words present in vocabulary. Two different model architectures that can be used by Word2Vec to create the word embeddings are the Continuous Bag of Words (CBOW) model(Used when dataset is small) & the Skip-Gram model(Used when the dataset is large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c60f15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'Word': [ 0.00973568 -0.00978038 -0.00649912  0.00278432  0.00643145 -0.00536809\n",
      "  0.00275295  0.00912222 -0.00681566 -0.00610047 -0.00498906 -0.00367669\n",
      "  0.00184911  0.0096832   0.0064373   0.00039713  0.00247128  0.00844069\n",
      "  0.00912851  0.00562807  0.00594696 -0.00762046 -0.00382721 -0.00568039\n",
      "  0.00618223 -0.00225665 -0.00877957  0.00761974  0.00839924 -0.00332041\n",
      "  0.00911608 -0.00073844 -0.00362591 -0.00038552  0.00019424 -0.00350519\n",
      "  0.00281408  0.00572937  0.00686912 -0.00890355 -0.00219323 -0.0054816\n",
      "  0.00752046  0.00650147 -0.00436071  0.00232704 -0.00595426  0.00023711\n",
      "  0.00946227 -0.00260905 -0.00518821 -0.00739698 -0.00291221 -0.00086426\n",
      "  0.00352841  0.00974166 -0.00338875  0.00190141  0.00968072  0.00153253\n",
      "  0.00098627  0.00980247  0.00929506  0.00770758 -0.00617046  0.00998438\n",
      "  0.00584908  0.00907294 -0.00199519  0.00335013  0.00683401 -0.00389338\n",
      "  0.00664285  0.00256217  0.00931417 -0.00303588 -0.00310872  0.00621545\n",
      " -0.00907828 -0.00725467 -0.00650033 -0.00074871 -0.00236262  0.00681588\n",
      "  0.00923622 -0.00090964  0.00141337  0.00202005 -0.00202002 -0.00803409\n",
      "  0.00744138 -0.00429792  0.00457686  0.00908978  0.00304382  0.00313903\n",
      "  0.00406123 -0.00270177  0.00382459  0.00033835]\n",
      "Most similar words to 'Word2Vec': [('for', 0.16378769278526306), ('love', 0.145950585603714), ('embeddings', 0.11074148863554001), ('useful', 0.09429629892110825), ('Word', 0.07480231672525406), ('embedding', 0.05048205703496933), ('natural', 0.04157735034823418), ('language', 0.03476494550704956), ('processing', 0.019152259454131126), ('NLP', 0.01613469049334526)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    ['I', 'love', 'natural', 'language', 'processing'],\n",
    "    ['Word2Vec', 'is', 'a', 'word', 'embedding', 'technique'],\n",
    "    ['It', 'learns', 'meaningful', 'representations', 'for', 'words'],\n",
    "    ['Word', 'embeddings', 'are', 'useful', 'in', 'NLP'],\n",
    "]\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# You can save the model to disk for later use\n",
    "model.save(\"word2vec_model.bin\")\n",
    "\n",
    "# To load the model from disk later\n",
    "# model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv['Word']\n",
    "print(\"Word vector for 'Word':\", word_vector)\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar('Word2Vec')\n",
    "print(\"Most similar words to 'Word2Vec':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222fae1",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "   ### a. Word embeddings eventually help in establishing the association of a word with another similar meaning word through the created vectors.\n",
    "   ### b. Captures symmantic meaning.\n",
    "   ### c. Low Dimensional vectors hence the computational time reduces.\n",
    "   ### d. Dense vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def90b6f",
   "metadata": {},
   "source": [
    "## Disadvantages:\n",
    "   ### a. Contexual meaning only captured within the window size. or in other word it has local context scope.\n",
    "   ### b. Not able to generate vectors for unseen words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa7b05",
   "metadata": {},
   "source": [
    "## 4. GloVe(Global Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca041b3",
   "metadata": {},
   "source": [
    "## It is also a Pre-trained word embedding technique used to overcome drawback of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a6bffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog': 0.9746318461970762\n",
      "Similarity between 'cat' and 'fish': 0.9594119455666702\n",
      "Similarity between 'dog' and 'bird': 0.9961498555841325\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe word embeddings (You can download these from the official GloVe website)\n",
    "# For demonstration purposes, we'll use a tiny example with just a few words.\n",
    "glove_embeddings = {\n",
    "    \"cat\": np.array([0.1, 0.2, 0.3]),\n",
    "    \"dog\": np.array([0.4, 0.5, 0.6]),\n",
    "    \"fish\": np.array([0.7, 0.8, 0.9]),\n",
    "    \"bird\": np.array([1.0, 1.1, 1.2])\n",
    "}\n",
    "\n",
    "def get_word_vector(word):\n",
    "    # Get the word vector from the GloVe embeddings\n",
    "    return glove_embeddings.get(word, np.zeros(3))  # Return zeros for unknown words\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    # Calculate the cosine similarity between two vectors\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "word1 = \"cat\"\n",
    "word2 = \"dog\"\n",
    "word3 = \"fish\"\n",
    "word4 = \"bird\"\n",
    "\n",
    "vector1 = get_word_vector(word1)\n",
    "vector2 = get_word_vector(word2)\n",
    "vector3 = get_word_vector(word3)\n",
    "vector4 = get_word_vector(word4)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {cosine_similarity(vector1, vector2)}\")\n",
    "print(f\"Similarity between '{word1}' and '{word3}': {cosine_similarity(vector1, vector3)}\")\n",
    "print(f\"Similarity between '{word2}' and '{word4}': {cosine_similarity(vector2, vector4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d874c",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "   ### a. Contexual meaning captured for both local and global scope.    b. Captures symmantic meaning.\n",
    "   ### c. Powerful than all previous wod embedding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c4f13",
   "metadata": {},
   "source": [
    "## Disadvantages:\n",
    "   ### a. Utilizes massive memory and takes time to load and train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c7550",
   "metadata": {},
   "source": [
    "## 5. BERT(Bidirectional Encoder Representations from Transformers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175f001",
   "metadata": {},
   "source": [
    "## BERT is the Pre-trained birectional trasformer for Language understanding. It has trained on 2500M Wikipedia words and 800M+ Books words. And BERT used by Google search Engine. BERT uses the encoder part of the Transformer, since it’s goal is to create a model that performs a number of different NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2ace76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a47a59f26a456c93b09ac05cfa614c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a80552a1b84c759c652f8a1611e6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64c319a0f7343e1a81e7698a016a2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3ff2a2f647420b875fb310e44bfe0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I really enjoyed the movie. It was fantastic!\n",
      "Predicted Label: 0\n",
      "Prediction Scores: tensor([[ 0.5397, -0.2734]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define the text you want to classify\n",
    "text = \"I really enjoyed the movie. It was fantastic!\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the prediction scores and predicted label\n",
    "predictions = outputs.logits\n",
    "predicted_label = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted Label: {predicted_label.item()}\")\n",
    "print(f\"Prediction Scores: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40a7a0",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "   ### a. Contexual meaning captured for both local and global scope.    \n",
    "   ### b. Captures symmantic meaning.\n",
    "   ### c. Powerful than all previous wod embedding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd602081",
   "metadata": {},
   "source": [
    "## Disadvantages:\n",
    "   ### a. Utilizes massive memory and takes time to load and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999175f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
